% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/importTagData.R
\name{importTagData}
\alias{importTagData}
\title{Import and Standardize Archival Biologging Tag Data}
\usage{
importTagData(
  data.folders,
  sensor.subdirectory = "CMD",
  wc.subdirectory = NULL,
  timezone = "UTC",
  import.mapping = NULL,
  axis.mapping = NULL,
  id.metadata,
  id.col = "ID",
  tag.model.col = "tag",
  tag.type.col = NULL,
  deploy.date.col = "tagging_date",
  deploy.lon.col = "deploy_lon",
  deploy.lat.col = "deploy_lat",
  pop.date.col = NULL,
  pop.lon.col = NULL,
  pop.lat.col = NULL,
  package.id.col = NULL,
  paddle.wheel.col = NULL,
  return.data = TRUE,
  save.files = FALSE,
  output.folder = NULL,
  output.suffix = NULL,
  data.table.threads = NULL,
  verbose = TRUE
)
}
\arguments{
\item{data.folders}{Character vector. Paths to the folders containing data to be processed.
Each folder corresponds to an individual animal and should contain subdirectories with sensor data and possibly
additional (Wildlife Computers) tag data.}

\item{sensor.subdirectory}{Character. Name of the subdirectory within each animal folder that contains sensor data (default: "CMD").
This subdirectory should include the sensor CSV files for the corresponding animal.}

\item{wc.subdirectory}{Character or NULL. Name of the subdirectory within each animal folder that contains Wildlife Computers tag data
(e.g., MiniPAT, MK10, or SPOT tag data), or NULL to auto-detect tag folders (default: NULL).
This subdirectory should contain the "Locations.csv" file with position data from the tag.}

\item{timezone}{Character string specifying the timezone for all datetime conversions.
Must be one of the valid timezones from \code{OlsonNames()}. Defaults to "UTC" (Coordinated Universal Time).}

\item{import.mapping}{Data frame or NULL. Optional specification of column names to import
from the CSV files and their corresponding sensor type and units. If \code{NULL} (default), uses
standard column names for archival G-Pilot and i-Pilot data. When specified, it must be a data frame
with three columns:
\itemize{
\item \strong{colname}: The exact column name as it appears in the input CSV file.
\item \strong{sensor}: The standardized sensor name to be used in the processed data. Valid options include: \code{"date"}, \code{"time"}, \code{"datetime"}, \code{"ax"}, \code{"ay"}, \code{"az"}, \code{"gx"}, \code{"gy"}, \code{"gz"}, \code{"mx"}, \code{"my"}, \code{"mz"}, \code{"depth"}, \code{"temp"}, \code{"paddle_freq"}, \code{"paddle_speed"}.
\item \strong{units}: The units of the sensor data. Valid options include: \code{"UTC"}, \code{"m/s2"}, \code{"g"}, \code{"mrad/s"}, \code{"rad/s"}, \code{"deg/s"}, \code{"uT"}, \code{"C"}, \code{"m"}, \code{"Hz"}, \code{"m/s"}, or \code{""} (an empty string) for unitless/dimensionless quantities.
}
For date and time columns, use \code{sensor = "datetime"} and \code{units = "UTC"}.}

\item{axis.mapping}{Optional. A data frame containing the axis transformations for the IMU (Inertial Measurement Unit).
This parameter is used to correctly configure the IMU axes to match the North-East-Down (NED) frame
or to mark faulty sensor data as NA.
The data frame should have four columns:
\itemize{
\item \emph{type}: A column specifying the tag type (\code{CAM} vs \code{CMD}).
\item \emph{tag}: A column specifying the tag or sensor identifier. The tags indicated in this column should match the tag types in the \code{id.metadata} data frame.
\item \emph{from}: A column indicating the original axis in the sensor's coordinate system.
\item \emph{to}: A column specifying the target axis in the desired coordinate system.
}
Both signal and swap transformations are allowed. Transformations can be defined for different tags in case multiple tags were used.}

\item{id.metadata}{Data frame. Metadata about the IDs to associate with the processed data.
Must contain at least columns for ID and tag type.}

\item{id.col}{Character. Column name for ID in \code{id.metadata} (default: "ID").}

\item{tag.model.col}{Character. Column name for the tag model in \code{id.metadata} (default: "tag").}

\item{tag.type.col}{Character string or \code{NULL}. The name of the column in \code{id.metadata} that specifies the tag's type (e.g., "Camera", "MS").
If \code{NULL} (default), the tag type is inferred from the animal \code{id} (e.g., by checking for substrings like \code{"CAM"}).
When specified, \code{axis.mapping} must include a 'type' column with values matching the unique tag types found in this \code{id.metadata} column.}

\item{deploy.date.col}{Character. Column name for the tagging date in \code{id.metadata} (default: "tagging_date").}

\item{deploy.lon.col}{Character. Column name for longitude in sensor data (default: "deploy_lon").}

\item{deploy.lat.col}{Character. Column name for latitude in sensor data (default: "deploy_lat").}

\item{pop.date.col}{Character or NULL. Column name for the popup date in \code{id.metadata}.
If NULL (default), popup date information will not be imported. When specified,
the column must contain POSIXct values.}

\item{pop.lon.col}{Character or NULL. Column name for popup longitude in \code{id.metadata}.
If NULL (default), popup longitude will not be imported. Must be specified
together with \code{pop.date.col} and \code{pop.lat.col} to enable popup location integration.}

\item{pop.lat.col}{Character or NULL. Column name for popup latitude in \code{id.metadata}.
If NULL (default), popup latitude will not be imported. Must be specified
together with \code{pop.date.col} and \code{pop.lon.col} to enable popup location integration.}

\item{package.id.col}{Character. Column name for the package id in \code{id.metadata} (default: "package_id").}

\item{paddle.wheel.col}{Character string specifying the name of the column in \code{id.metadata}
that indicates whether each tag was equipped with a magnetic paddle wheel for speed estimation.
The column should contain logical values (TRUE/FALSE) or binary values (1/0) where 1/TRUE indicates
presence of a paddle wheel. If NULL (default), the function assumes no paddle wheel data needs to be processed.}

\item{return.data}{Logical. Controls whether the function returns the processed data
as a list in memory. When processing large or numerous datasets, set to \code{FALSE} to reduce
memory usage. Note that either \code{return.data} or \code{save.files} must be \code{TRUE}
(or both). Default is \code{TRUE}.}

\item{save.files}{Logical. If \code{TRUE}, the processed data for each ID will be saved as RDS files
during the iteration process. This ensures that progress is saved incrementally, which can
help prevent data loss if the process is interrupted or stops midway. Default is \code{FALSE}.}

\item{output.folder}{Character. Path to the folder where the processed files will be saved.
This parameter is only used if \code{save.files = TRUE}. If \code{NULL}, the RDS file will be saved
in the data folder corresponding to each ID. Default is \code{NULL}.}

\item{output.suffix}{Character. A suffix to append to the file name when saving.
This parameter is only used if \code{save.files = TRUE}.}

\item{data.table.threads}{Integer or NULL. Specifies the number of threads
that data.table should use for parallelized operations. NULL (default): Uses data.table's current default threading.
Notes:
\itemize{
\item Optimal thread count depends on your CPU cores and data size
\item More threads use more RAM but can significantly speed up large operations
\item Can be permanently set via \code{data.table::setDTthreads()}
\item Current thread count: \code{data.table::getDTthreads()}
}}

\item{verbose}{Logical. If TRUE, the function will print detailed processing information. Defaults to TRUE.}
}
\value{
If \code{return.data = TRUE}, returns a list where each element contains the
processed sensor data for an individual folder. If \code{return.data = FALSE},
returns \code{NULL} invisibly. In all cases, data will be saved to disk if
\code{save.files = TRUE}.
}
\description{
This function provides an end-to-end solution for importing and standardizing
high-resolution biologging data from archival tags. It handles raw sensor outputs from
various tag types (defaulting to G-Pilot and i-Pilot formats) and transforms them into
analysis-ready datasets with consistent formatting.

It can process data from a single individual or automatically iterate through directories
to process data from multiple individuals sequentially, making it well-suited for
large-scale studies.

Sensor time series are automatically processed and converted to standardized units
— \code{g} for acceleration, \code{rad/s} for angular velocity, and \code{µT} for magnetic fields —
while preserving the original temporal resolution and structure.

Users can optionally specify a custom column mapping using the \code{import.mapping} argument,
which allows importing data from CSV files with non-standard column names. When provided,
this mapping defines how columns in the raw data correspond to standard sensor types,
including the associated units. This ensures compatibility with the function's
standardization routines, even when input data deviates from default naming conventions.

Additionally, the function includes options for axis transformations to
convert IMU data into animal-centric coordinate frames, such as NED (North-East-Down).

When available, location data from Wildlife Computers tags (e.g., MiniPAT, MK10, SPOT)
can be integrated automatically. These should be stored in dedicated subfolders
(e.g., \code{"SPOT"}) within each individual's directory. Deployment and pop-up locations,
if present in the metadata, are also extracted and included in the output.

The function employs memory-efficient processing to handle large datasets (>10 million rows),
using optimized \code{data.table} operations with configurable multi-threading.
}
\note{
For optimal performance with very large datasets, consider:
\itemize{
\item Setting \code{data.table.threads} to match available CPU cores;
\item Using \code{save.files = TRUE} to reduce memory usage;
\item Processing individuals sequentially rather than simultaneously.
}
}
\seealso{
\link{importTagData}
}
