% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/renderOverlayVideo.R
\name{renderOverlayVideo}
\alias{renderOverlayVideo}
\title{renderOverlayVideo}
\usage{
renderOverlayVideo(
  video.file,
  video.metadata,
  sensor.data,
  output.file,
  output.directory = NULL,
  start.time = NULL,
  end.time = NULL,
  duration = NULL,
  overlay.side = "left",
  depth.window = 5 * 60,
  vedba.window = 30,
  vertical.speed.window = 30,
  tailbeat.window = 30,
  text.color = "black",
  sensor.val.color = "red3",
  jpeg.quality = 3,
  video.compression = "h265",
  crf = 28,
  cores = 1
)
}
\arguments{
\item{video.file}{The path to the input video file.}

\item{video.metadata}{A data frame containing metadata about the video,
including start and end times, frame rate, and video ID. This should be
the output of the \code{\link{getVideoMetadata}} function.}

\item{sensor.data}{A data frame containing the sensor data, which includes timestamps and the associated
metrics to be overlayed (e.g., depth, heading, pitch, roll).}

\item{output.file}{The name of the output video file (e.g., "output.mp4").}

\item{output.directory}{The directory where temporary and final files will be saved.}

\item{start.time}{(Optional) The start time in the video from which to begin processing,
in the format "HH:MM:SS". Defaults to "00:00:00".}

\item{end.time}{(Optional) The end time in the video up to which processing should occur,
in the format "HH:MM:SS".}

\item{duration}{(Optional) The duration of the video segment to process, in seconds. Overrides \code{end.time}
if specified.}

\item{overlay.side}{The side of the frame where overlays should be displayed ("left", "center" or "right"). Defaults to "left".}

\item{depth.window}{The time window (in seconds) for averaging or visualizing depth data. Defaults to 300 seconds (5 minutes).}

\item{vedba.window}{The time window (in seconds) for averaging or visualizing VE-DBA (Vectorial Dynamic Body Acceleration) data.
Defaults to 30 seconds.}

\item{vertical.speed.window}{The time window (in seconds) for calculating vertical speed. Defaults to 30 seconds.}

\item{tailbeat.window}{The time window (in seconds) for calculating and visualizing tail beat frequency. Defaults to 30 seconds.}

\item{text.color}{The color of the overlay text. Defaults to "black".}

\item{sensor.val.color}{The color used to display sensor values. Defaults to "red3".}

\item{jpeg.quality}{An integer specifying the quality of the extracted JPEG frames.
This value controls the level of compression applied to the frames.
The \code{jpeg.quality} value is passed to \code{ffmpeg}'s \code{-qscale:v} option. A value of
\code{1} represents the best quality (larger file size), while a value of \code{31}
represents the worst quality (smaller file size). The default is \code{4}, which provides
a good balance between quality and file size.}

\item{video.compression}{Character. Compression format for the output video. Options are \code{"h264"} for standard compression
and \code{"h265"} for HEVC compression. Default is \code{"h265"}.
HEVC (H.265) offers higher compression efficiency compared to H.264, resulting in smaller file sizes
with comparable or better video quality.}

\item{crf}{A numeric value for the Constant Rate Factor (CRF), which controls the quality and file size of the output video.
The CRF range is from 0 to 51, where:
\itemize{
\item A value of 0 represents lossless encoding, resulting in the highest quality but the largest file size.
\item A value of 28 is considered a good balance between quality and compression for most use cases. This is the default value.
\item A higher value (closer to 51) will result in lower quality and smaller file sizes.
}}

\item{cores}{The number of processor cores to use for parallel computation. Defaults to 1 (single-core).}
}
\value{
A video file with synchronized sensor overlays saved to the specified \code{output.file}.
}
\description{
This function generates a video with synchronized sensor data overlayed onto the video frames.
It processes the video and sensor data, extracts the frames, synchronizes them with the sensor data,
applies overlays, and outputs the final annotated video.
}
\details{
This function extracts frames from the specified video, aligns each frame with the corresponding timestamp in the
sensor data, and overlays the sensor data onto the frames. It supports single-core and multi-core processing
for frame overlay generation. The final annotated video is assembled using FFmpeg.
}
\note{
Ensure FFmpeg is installed and accessible via the system PATH, as it is required for video processing.
}
